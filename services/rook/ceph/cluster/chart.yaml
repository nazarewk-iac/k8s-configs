# HelmChartInflationGenerator docs are wrong, see https://github.com/kubernetes-sigs/kustomize/blob/b0d7721049d557063e324c7c2c1ef45bb73e4bdc/api/types/helmchartargs.go#L33-L75
apiVersion: builtin
kind: HelmChartInflationGenerator
metadata:
  name: rook-ceph-cluster

name: rook-ceph-cluster
repo: https://charts.rook.io/release
version: v1.8.8

releaseName: rook-ceph-cluster
namespace: rook-ceph

valuesInline:
  toolbox:
    enabled: true

  clusterName: rook-ceph-nazarewk-krul
  cephClusterSpec:
    dataDirHostPath: /var/lib/rook/rook-ceph-nazarewk-krul

    mon:
      count: 1
      allowMultiplePerNode: true  # running single node for now

    storage:
      useAllNodes: true
      useAllDevices: false
      config:
        encryptedDevice: 'true'
      storageClassDeviceSets:
        - name: rook-ceph-nvme
          count: 1
          portable: false
          encrypted: true
          volumeClaimTemplates:
            - metadata:
                labels:
                  storage.nazarewk.pw/device-class: nvme

              spec:
                storageClassName: manual-rook-ceph-cluster
                resources:
                  requests:
                    storage: 1020G
                volumeMode: Block
                accessModes:
                  - ReadWriteOnce
        - name: rook-ceph-ssd
          count: 2
          portable: false
          encrypted: true
          volumeClaimTemplates:
            - metadata:
                labels:
                  storage.nazarewk.pw/device-class: ssd
              spec:
                storageClassName: manual-rook-ceph-cluster
                resources:
                  requests:
                    storage: 2040G
                volumeMode: Block
                accessModes:
                  - ReadWriteOnce
        - name: rook-ceph-hdd
          count: 2
          portable: false
          encrypted: true
          volumeClaimTemplates:
            - metadata:
                labels:
                  storage.nazarewk.pw/device-class: hdd

              spec:
                storageClassName: manual-rook-ceph-cluster
                resources:
                  requests:
                    storage: 4040G
                volumeMode: Block
                accessModes:
                  - ReadWriteOnce

    resources:
      mon:
        limits:
          cpu: 4
          memory: 4Gi
    healthCheck:
      daemonHealth:
        mon:
          disabled: false
          interval: 2m50s
          timeout: 30m
      livenessProbe:
        mon:
          disabled: false
          probe: &livenessProbe
            timeoutSeconds: &timeout 60
            periodSeconds: &period 15
            failureThreshold: &livenessFailure 4
        mgr:
          disabled: false
        osd:
          disabled: false
      startupProbe:
        mon:
          disabled: false
          probe: &startupProbe
            periodSeconds: *period
            timeoutSeconds: 5
            initialDelaySeconds: 10
            failureThreshold: 240
        mgr:
          disabled: false
          probe: *startupProbe
        osd:
          disabled: false
          probe: *startupProbe

  configOverride: |
    [global]
    mon_allow_pool_delete = true
    osd_pool_default_size = 1
    osd_pool_default_min_size = 1

  cephBlockPools:
    ## single replica NVMe pool doesn't work at all
    #- name: nvme-1-replicas
    #  spec:
    #    deviceClass: nvme
    #    failureDomain: osd
    #    replicated:
    #      size: 1
    #      requireSafeReplicaSize: false
    #  storageClass:
    #    name: nvme-1-replicas
    #    enabled: true
    #    isDefault: false
    #    allowVolumeExpansion: true
    #    reclaimPolicy: Delete

    - name: ssd-2-replicas
      spec:
        deviceClass: ssd
        failureDomain: osd
        replicated:
          size: 2
      storageClass:
        name: ssd-2-replicas
        enabled: true
        isDefault: false
        allowVolumeExpansion: true
        reclaimPolicy: Delete

    - name: hdd-2-replicas
      spec:
        deviceClass: hdd
        failureDomain: osd
        replicated:
          size: 2
      storageClass:
        name: hdd-2-replicas
        enabled: true
        isDefault: false
        allowVolumeExpansion: true
        reclaimPolicy: Delete

    - name: fast-2-replicas
      spec:
        failureDomain: osd
        replicated:
          size: 2
          hybridStorage:
            primaryDeviceClass: nvme
            secondaryDeviceClass: ssd
      storageClass:
        name: fast-2-replicas
        enabled: true
        isDefault: false
        allowVolumeExpansion: true
        reclaimPolicy: Delete

    - name: fast-3-replicas
      spec:
        failureDomain: osd
        replicated:
          size: 3
          hybridStorage:
            primaryDeviceClass: nvme
            secondaryDeviceClass: ssd
      storageClass:
        name: fast-3-replicas
        enabled: true
        isDefault: false
        allowVolumeExpansion: true
        reclaimPolicy: Delete

    - name: medium-2-replicas
      spec:
        failureDomain: osd
        replicated:
          size: 2
          hybridStorage:
            primaryDeviceClass: ssd
            secondaryDeviceClass: hdd
      storageClass:
        name: medium-2-replicas
        enabled: true
        isDefault: false
        allowVolumeExpansion: true
        reclaimPolicy: Delete

    - name: medium-3-replicas
      spec:
        failureDomain: osd
        replicated:
          size: 3
          hybridStorage:
            primaryDeviceClass: ssd
            secondaryDeviceClass: hdd
      storageClass:
        name: medium-3-replicas
        enabled: true
        isDefault: false
        allowVolumeExpansion: true
        reclaimPolicy: Delete

    - name: medium-4-replicas
      spec:
        failureDomain: osd
        replicated:
          size: 4
          hybridStorage:
            primaryDeviceClass: ssd
            secondaryDeviceClass: hdd
      storageClass:
        name: medium-4-replicas
        enabled: true
        isDefault: false
        allowVolumeExpansion: true
        reclaimPolicy: Delete

  cephFileSystems: []
  cephObjectStores: []
